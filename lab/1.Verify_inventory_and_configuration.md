### **1. Verify Inventory and Configuration**

Before troubleshooting further, ensure that your Ansible inventory and configuration files are correctly set up.

### **1.1. Check Inventory File**

Verify that the Ansible inventory file (`/home/vagrant/ansible/inventory`) is correctly formatted.

**Command:**

```bash
cat /home/vagrant/ansible/inventory

```

**Expected Output:**

```
[dev]
node1.ansi.example.com ansible_host=192.168.56.201 ansible_user=vagrant ansible_ssh_private_key_file=/home/vagrant/.ssh/id_rsa

[test]
node2.ansi.example.com ansible_host=192.168.56.202 ansible_user=vagrant ansible_ssh_private_key_file=/home/vagrant/.ssh/id_rsa

[prod]
node3.ansi.example.com ansible_host=192.168.56.203 ansible_user=vagrant ansible_ssh_private_key_file=/home/vagrant/.ssh/id_rsa

[balancers]
node4.ansi.example.com ansible_host=192.168.56.204 ansible_user=vagrant ansible_ssh_private_key_file=/home/vagrant/.ssh/id_rsa

[repo]
repo.ansi.example.com ansible_host=192.168.56.199 ansible_user=vagrant ansible_ssh_private_key_file=/home/vagrant/.ssh/id_rsa

[control]
control.ansi.example.com ansible_host=192.168.56.200 ansible_user=vagrant ansible_ssh_private_key_file=/home/vagrant/.ssh/id_rsa

[all:children]
dev
test
prod
balancers

[webservers:children]
repo

```

**Things to Check:**

- Ensure that the IP addresses and SSH user (`ansible_user`) are correct.
- Verify that the SSH key file paths (`ansible_ssh_private_key_file`) are accurate.

### **1.2. Check Ansible Configuration**

Make sure that your Ansible configuration file (`ansible.cfg`) is properly set.

**Command:**

```bash
cat /home/vagrant/ansible/ansible.cfg

```

**Expected Output:**

```
[defaults]
host_key_checking = false
inventory = /home/vagrant/ansible/inventory
deprecation_warnings = false
stdout_callback = yaml
scp_if_ssh = true
system_warnings = false
command_warnings = false
remote_tmp = /tmp
forks = 10
force_handlers = True
timeout = 30

[ssh_connection]
pipelining = True

[privilege_escalation]
become = True
become_method = sudo
become_user = root
become_ask_pass = False

```

**Additional Check:**
Make sure Ansible is using this configuration file by running the following command:

```bash
ansible --version

```

**Expected Output:**

```
ansible 2.x.x
  config file = /home/vagrant/ansible/ansible.cfg
  ...

```

---

If you are facing this error

```jsx
Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).

```

### **1. Verify the Existence of the `control` User on the VM**

**Access the Control Node via Vagrant:**

```bash
vagrant ssh control

```

**Check if the `control` User Exists:**

```bash
getent passwd control

```

- **If the user exists**, you'll see a line with user information.
- **If the user does not exist**, create it:
    
    ```bash
    sudo adduser control
    
    ```
    

**Set a Password for the vagrant User:**

```bash
sudo passwd vagrant

```

Enter a secure password when prompted.

### **1-1. Enable Password Authentication on the SSH Server**

**Edit the SSH Configuration File:**

```bash
sudo nano /etc/ssh/sshd_config

```

**Modify the Following Settings:**

- **Uncomment and set** `PasswordAuthentication` to `yes`:
    
    ```
    PasswordAuthentication yes
    
    ```
    
- **Ensure** `PermitRootLogin` is set to `no` for security:
    
    ```
    PermitRootLogin no
    
    ```
    

**Save and Exit:**

- Press `Ctrl + X`, then `Y`, and `Enter` to save changes.

**Restart the SSH Service:**

```bash
sudo systemctl restart sshd

```

### **2. Manually Restore SSH Access**

Since SSH access through Ansible is no longer possible, we will manually restore SSH access on each managed node.

### **2.1. SSH into Each Managed Node Manually**

You will need to use Vagrant to manually SSH into each managed node. Execute the following commands one by one:

```bash
vagrant ssh node1
vagrant ssh node2
vagrant ssh node3
vagrant ssh node4
vagrant ssh repo
vagrant ssh control

```

**Note:** If SSH access fails here, you may need to troubleshoot Vagrant configuration or consider recreating the virtual machines.

### **2.2. Restore the Vagrant Insecure Public Key**

Once logged into each managed node, you will need to restore the Vagrant insecure public key to the `authorized_keys` file.

1. **Check the Current `authorized_keys` File:**
    
    ```bash
    cat /home/vagrant/.ssh/authorized_keys
    
    ```
    
    **Expected Output:**
    The file should contain the Vagrant insecure public key. If it does not, we will manually add it.
    
2. **Add the Vagrant Insecure Public Key:**
    
    To restore SSH access, you need to add the Vagrant insecure public key to the `authorized_keys` file. Run the following command:
    
    ```bash
    echo "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCy7Lkn6+6OTcGtyDLuxAqZ9Th58Ee30S2dzMvLy9XjEsIqdCbf+u0a2Wuhkda6ZC5j86ohppZPVjZBxRHVwBrv+aItwJzVSCrDJy1yxTo/dk+0V5LpEsx2P+3b40fx3rGWlQgT7+eaQmHH+49iJg4cUaVqGmBJSLx3IVFcLtwkZXMld/GEekr7WyOqqrG9ZNTk0PiMrz2PbhP2Y1ThTh6h74clilRs8HqnbIzJe6PyPc/VXWp0IyhIg8zOGorPuz8L91iQQtdZjo/7r3XZLkG0eStLqK3VtBZ5x+s7LwMb1ltIovL70akCQj4E11w5m7tK1KKtHzjL3rkzBWWqEKX5Qir vagrant@host" >> /home/vagrant/.ssh/authorized_keys
    
    ```
    
3. **Set Correct Permissions:**
    
    For security, ensure the correct permissions are set on the `.ssh` directory and the `authorized_keys` file.
    
    ```bash
    chmod 700 /home/vagrant/.ssh
    chmod 600 /home/vagrant/.ssh/authorized_keys
    chown vagrant:vagrant /home/vagrant/.ssh /home/vagrant/.ssh/authorized_keys
    
    ```
    
    **Explanation:**
    
    - The `.ssh` directory should have `700` permissions.
    - The `authorized_keys` file should have `600` permissions.
    - The `vagrant` user should own both the `.ssh` directory and the `authorized_keys` file.
4. **Repeat for All Managed Nodes:**
    
    Repeat these steps for all managed nodes (`node1` to `node4`, `repo`, and `control`).
    

### **2.3. Verify SSH Access from the Control Node**

Once you've restored the Vagrant insecure public key on all managed nodes, SSH access should be restored. Now, verify that you can SSH from the control node to each managed node.

1. **Access the Control Node:**
    
    ```bash
    vagrant ssh control
    
    ```
    
2. **Test SSH Access to Each Managed Node:**
    
    From the control node, test SSH access to each managed node using the following commands:
    
    ```bash
    ssh vagrant@192.168.56.201
    ssh vagrant@192.168.56.202
    ssh vagrant@192.168.56.203
    ssh vagrant@192.168.56.204
    ssh vagrant@192.168.56.199
    ssh vagrant@192.168.56.200
    
    ```
    
    **Expected Result:**
    You should be able to log in without being prompted for a password.
    
3. **If SSH Fails:**
    - Double-check the content and permissions of the `authorized_keys` file.
    - Ensure that the Vagrant private key (`id_rsa`) on the control node matches the public key on the managed nodes.

---

# **4. Testing Ansible Connection (Using the Ping Module)**

To verify the connection between the Control Node and the Managed Nodes, use the Ansible `ping` module.

**Command:**

```bash
ansible all -m ping

```

- `ansible`: Executes the Ansible command.
- `all`: Targets all hosts defined in the inventory file.
- `m ping`: Uses the `ping` module to check connectivity and get a response from the hosts.

**Expected Output:**

```
repo.ansi.example.com | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
control.ansi.example.com | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
node1.ansi.example.com | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
node2.ansi.example.com | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
node3.ansi.example.com | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
node4.ansi.example.com | SUCCESS => {
    "changed": false,
    "ping": "pong"
}

```

- If you receive a `"ping": "pong"` response from each host, the connection is correctly set up.

**Troubleshooting:**

- If you see an `UNREACHABLE` status, check the SSH connection settings or network configuration.
- If an `AUTHENTICATION FAILURE` occurs, verify your SSH key setup.

---

# **5. Running a Playbook**

An Ansible playbook defines a series of tasks to automate processes. For example, let's write a playbook to install an Apache web server on the nodes in the `prod` group.

**Playbook Path:** `/home/vagrant/ansible/setup_webservers.yml`

**Creating the Playbook:**

```bash
vi /home/vagrant/ansible/setup_webservers.yml

```

**Playbook Content:**

```yaml
---
- name: Setup Apache on all webservers
  hosts:
    - dev
    - test
    - prod
  become: yes
  tasks:
    - name: Install Apache
      yum:
        name: httpd
        state: present

    - name: Start and enable Apache
      service:
        name: httpd
        state: started
        enabled: yes

```

**Explanation:**

- `name`: Describes the playbook or task name.
- `hosts`: Specifies the group of hosts where the playbook will be executed (e.g., `webservers` group).
- `become: yes`: Executes the command with elevated privileges.
- `tasks`: Defines the list of tasks to be executed.
    - **Task 1: Installing Apache**
        - Uses the `yum` module to install the `httpd` package.
        - `state: present`: Installs the package if it is not already installed, otherwise does nothing.
    - **Task 2: Starting and Enabling Apache**
        - Uses the `service` module to start the `httpd` service and ensure it starts at boot.
        - `state: started`: Ensures the service is running.
        - `enabled: yes`: Ensures the service starts automatically at boot.

**Running the Playbook:**

```bash
ansible-playbook /home/vagrant/ansible/setup_webservers.yml

```

**Expected Output:**

```
PLAY [Setup Apache on all webservers] *****************************************

TASK [Gathering Facts] *********************************************************
ok: [node3.ansi.example.com]
ok: [node4.ansi.example.com]

TASK [Install Apache] *********************************************************
changed: [node3.ansi.example.com]
changed: [node4.ansi.example.com]

TASK [Start and enable Apache] *************************************************
changed: [node3.ansi.example.com]
changed: [node4.ansi.example.com]

PLAY RECAP *********************************************************************
node3.ansi.example.com     : ok=3    changed=2    unreachable=0    failed=0
node4.ansi.example.com     : ok=3    changed=2    unreachable=0    failed=0

```

- **Explanation:**
    - `ok`: The task was successfully executed.
    - `changed`: Something was changed during the task (e.g., Apache was installed or started).
    - `unreachable`: The host could not be reached.
    - `failed`: The task failed to complete.

**Verification:**
Check if the Apache web server was successfully installed and is running.

```bash
systemctl status httpd

```

- The service status should show `active (running)`.

---

1. **Firewall Settings:**
    - Ensure that the firewall on the Managed Nodes allows SSH access.
    - Example:
    
    ```bash
    sudo firewall-cmd --permanent --add-service=ssh
    sudo firewall-cmd --reload
    
    ```
